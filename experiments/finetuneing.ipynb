{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afreens/miniconda3/envs/anlp_assn2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(file):\n",
    "    examples = []\n",
    "    # example = {col: [] for col in INPUT_COLUMNS}\n",
    "    idx = 0\n",
    "    example = {\"id\":idx, \"tokens\": [], \"ner_tags\":[]}\n",
    "    \n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\\n\" or not line:\n",
    "                assert len(example[\"tokens\"]) == len(example[\"ner_tags\"])\n",
    "                examples.append(example)\n",
    "                idx+=1\n",
    "                example = {\"id\":idx, \"tokens\": [], \"ner_tags\":[]}\n",
    "            else:\n",
    "                row_cols = line.split()\n",
    "                assert len(row_cols) == 4\n",
    "                example[\"tokens\"].append(row_cols[0])\n",
    "                example[\"ner_tags\"].append(row_cols[-1])\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(dataset_paths):\n",
    "    all_data  =sum([read_conll(x) for x in dataset_paths], [])\n",
    "    train_data, test_data = train_test_split(all_data)\n",
    "    ner_feature = datasets.Sequence(\n",
    "                        datasets.features.ClassLabel(\n",
    "                            names=[\n",
    "                                \"O\",\n",
    "                                \"B-MethodName\",\n",
    "                                \"I-MethodName\",\n",
    "                                \"B-HyperparameterName\",\n",
    "                                \"I-HyperparameterName\",\n",
    "                                \"B-HyperparameterValue\",\n",
    "                                \"I-HyperparameterValue\",\n",
    "                                \"B-MetricName\",\n",
    "                                \"I-MetricName\",\n",
    "                                \"B-MetricValue\",\n",
    "                                \"I-MetricValue\",\n",
    "                                \"B-TaskName\",\n",
    "                                \"I-TaskName\",\n",
    "                                \"B-DatasetName\",\n",
    "                                \"I-DatasetName\"\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    token_feature = datasets.Sequence(datasets.Value(\"string\"))\n",
    "    id_feature = datasets.Value(\"string\")\n",
    "    train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=train_data), features=datasets.Features({\n",
    "        \"id\":id_feature,\n",
    "        \"ner_tags\":ner_feature,\n",
    "        \"tokens\" : token_feature\n",
    "    }))\n",
    "    test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=test_data), features=datasets.Features({\n",
    "        \"id\":id_feature,\n",
    "        \"ner_tags\":ner_feature,\n",
    "        \"tokens\" : token_feature\n",
    "    }))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "dataset_paths = \"/home/afreens/projects/nlp_from_scratch_assignment/labelled_data/*\"\n",
    "train_dataset, test_dataset = get_dataset(glob.glob(dataset_paths))\n",
    "task = \"ner\"\n",
    "label_list = train_dataset.features[f\"{task}_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "# model_checkpoint = \"/data/user_data/afreens/roberta-large-conll2003-finetuned-ner1/checkpoint-1500\"\n",
    "model_checkpoint = \"/data/user_data/afreens/checkpoint-1500-finetuned-ner_v2/checkpoint-1500/\"\n",
    "batch_size = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Ä We', 'Ä evaluate', 'Ä the', 'Ä statistical', 'Ä significance', 'Ä of', 'Ä the', 'Ä pair', 'wise', 'Ä differences', 'Ä in', 'Ä the', 'Ä proportions', 'Ä of', 'Ä correct', 'Ä and', 'Ä halluc', 'inated', 'Ä translations', 'Ä using', 'Ä two', 'Ä -', 'Ä sided', 'Ä Student', 'Ä test', 'Ä for', 'Ä two', 'Ä related', 'Ä samples', 'Ä with', 'Ä 5', 'Ä %', 'Ä confidence', 'Ä level', 'Ä .', 'Ä We', 'Ä provide', 'Ä more', 'Ä details', 'Ä on', 'Ä the', 'Ä annotation', 'Ä guidelines', 'Ä and', 'Ä inter', 'Ä -', 'Ä annotation', 'Ä agreement', 'Ä in', 'Ä Appendix', '</s>']\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[4]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 52\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, None]\n"
     ]
    }
   ],
   "source": [
    "print(len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"]))\n",
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 58\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 39848, 9, 42806, 4052, 8738, 449, 20057, 111, 14077, 7753, 11282, 5, 4472, 227, 10, 25860, 37681, 8, 762, 44493, 634, 42806, 25, 1602, 11, 7162, 155, 4, 176, 479, 20, 5838, 9, 42806, 11, 305, 11674, 108, 1646, 926, 111, 2271, 16, 7646, 11, 9513, 361, 479, 20, 775, 311, 14, 22, 885, 1589, 42806, 22, 16, 3667, 204, 7, 195, 498, 3845, 87, 22, 885, 1589, 1021, 42806, 22, 479, 2], [0, 96, 42, 173, 2156, 52, 5393, 442, 20181, 3693, 11857, 26492, 7891, 268, 55, 5693, 1241, 5, 24934, 1938, 9, 49, 23341, 8, 30264, 1635, 479, 28256, 2787, 5, 23341, 9, 10, 26739, 1546, 16, 5616, 13, 26640, 8, 2386, 5, 1421, 7, 28, 10696, 55, 14146, 479, 635, 2156, 26640, 1937, 473, 45, 1888, 44042, 1042, 187, 5, 1546, 30264, 1635, 240, 7, 28, 43547, 11, 455, 15339, 479, 28256, 2787, 258, 23341, 8, 30264, 1635, 2386, 44042, 7, 28, 3744, 19, 795, 15339, 2156, 2905, 981, 7, 1233, 5838, 3077, 6122, 15, 5, 24934, 1938, 672, 8, 6554, 5574, 479, 28256, 2787, 26739, 4836, 33, 10, 251, 750, 2156, 8, 1533, 1364, 33, 3751, 7, 24934, 2072, 1198, 111, 5389, 7891, 268, 23, 1337, 24934, 1938, 1389, 36, 12242, 4400, 1076, 479, 2156, 4400, 1076, 479, 2156, 4400, 1076, 479, 2156, 4400, 1076, 479, 2156, 8835, 4839, 479, 1993, 9, 42, 173, 7235, 15, 9689, 15362, 111, 129, 3092, 36, 4412, 163, 18854, 4839, 13, 3645, 8, 19233, 20257, 8558, 479, 28256, 2787, 2788, 2706, 3092, 34, 3489, 57, 11394, 25, 10, 55, 1202, 3685, 36, 18775, 282, 1071, 4400, 1076, 479, 2156, 4400, 1076, 479, 2156, 8157, 4839, 528, 7, 5, 739, 4195, 32644, 8, 29698, 46133, 479, 12845, 173, 34, 19737, 42, 936, 2156, 600, 129, 13, 10439, 24934, 1938, 1389, 36, 159, 7, 290, 111, 828, 30264, 1635, 4839, 8, 19, 4281, 1282, 479, 2], [0, 211, 5457, 25522, 211, 112, 2156, 1347, 1347, 1347, 2156, 211, 255, 35524, 2156, 147, 5, 449, 111, 3553, 3685, 211, 449, 5457, 3023, 449, 939, 2156, 1423, 449, 939, 234, 449, 2], [0, 3023, 5457, 646, 3023, 112, 2156, 3023, 132, 2156, 1347, 1347, 1347, 2156, 3023, 295, 27779, 7, 120, 37617, 1538, 30464, 13, 349, 19233, 1368, 5457, 646, 1368, 112, 2156, 1368, 132, 2156, 1347, 1347, 1347, 2156, 1368, 295, 27779, 2156, 2], [0, 166, 10516, 5, 17325, 11382, 9, 5, 1763, 10715, 5550, 11, 5, 27311, 9, 4577, 8, 40510, 9339, 41762, 634, 80, 111, 26710, 9067, 1296, 13, 80, 1330, 7931, 19, 195, 7606, 2123, 672, 479, 166, 694, 55, 1254, 15, 5, 47760, 6401, 8, 3222, 111, 47760, 1288, 11, 40643, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13, 13, 14, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 12, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2460 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2460/2460 [00:00<00:00, 2735.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 821/821 [00:00<00:00, 3689.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(821, 2460)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_test_dataset), len(tokenized_train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from zmq import device\n",
    "from torch import nn\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), ignore_mismatched_sizes=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model= nn.DataParallel(model,device_ids = [1, 3])\n",
    "model = model.to(device)\n",
    "\n",
    "# tokenized_train_dataset.to(device)\n",
    "# tokenized_test_dataset.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"/data/user_data/afreens/{model_name}-finetuned-{task}_v2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit = 1\n",
    "    # per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=4\n",
    ")\n",
    "\n",
    "# args.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624765/3199598991.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MetricName': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'MetricValue': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_test_dataset[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
